{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #1: Train an raw sound classification model with Azure Machine Learning\n",
    "In this tutorial, you train a machine learning model on remote compute resources. You'll use the training and deployment workflow for Azure Machine Learning service (preview) in a Python Jupyter notebook. You can then use the notebook as a template to train your own machine learning model with your own data. This tutorial is part one of a two-part tutorial series.\n",
    "\n",
    "This tutorial trains a CNN model using raw sound dataset captured by [SoundCaptureModule](https://github.com/ms-iotkithol-jp/MicCaptureIoTSoundSample) on Azure IoT Edge device with Azure Machine Learning. Sound dataset consist from raw format and csv format file for each timespan. The goal is to create a multi-class classifier to identify the major or minor code of guiter equipment.\n",
    "\n",
    "Learn how to:\n",
    "\n",
    "- Set up your development environment\n",
    "- Access and examine the data\n",
    "- Train a CNN model on a remote cluster\n",
    "- Review training results, find and register the best model\n",
    "- You'll learn how to select a model and deploy it in part two of this tutorial later.\n",
    "\n",
    "## Prerequisites\n",
    "See prerequisites in the Azure Machine Learning documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "All the setup for your development work can be accomplished in a Python notebook. Setup includes:\n",
    "\n",
    "- Importing Python packages\n",
    "- Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "- Creating an experiment to track all your runs\n",
    "- Creating a remote compute target to use for training\n",
    "- Import packages\n",
    "- Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-1\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to workspace\n",
    "Create a workspace object from the existing workspace. Workspace.from_config() reads the file config.json and loads the details into an object named ws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment\n",
    "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'raw-sound-major-miner-cnn'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify dataset and testset names and positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-2\n",
    "dataset_name = 'sound_data'\n",
    "testset_name = 'sound_test'\n",
    "\n",
    "data_folder_path = 'data-wav'\n",
    "test_folder_path = 'test-wav'\n",
    "\n",
    "soundDataDefFile = 'sounddata.yml'\n",
    "dataSrorageConfigFile = 'data-storage-config.yml'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "<b>Creation of compute takes approximately 5 minutes</b>. If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D3_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the necessary packages and compute resources to train a model in the cloud.\n",
    "\n",
    "## Explore data\n",
    "Before you train a model, you need to understand the data that you are using to train it. In this section you learn how to:\n",
    "\n",
    "- Download the captured raw sound is CSV files dataset\n",
    "- Split each csv file into chunks when captured and convert it to a format that is suitable for CNN training \n",
    "- Create label dataset using user specified csv file. the csv file should be provided by user apart from sound csv files.\n",
    "- Display some sounds\n",
    "\n",
    "The format of sound scv file name is... \n",
    "<i>deviceid</i>-sound-<i>yyyyMMddHHmmssffffff</i>.csv\n",
    "The part of <i>yyyyMMddHHmmss</i> means that the start time of sound capturing so that user can specify the appropriate files by specifying the start and end times.\n",
    "So the csv file for label dataset specification consist from following format record.\n",
    "<i>label_name</i>,<i>start timestamp</i>,<i>end timestamp</i>\n",
    "\n",
    "both timestamp format shold be following.\n",
    "<i>yyyy</i>/<i>MM</i>/<i>dd</i> <i>HH</i>:<i>mm</i>:<i>ss</i>\n",
    "\n",
    "In the following code label dataset definition csv files name are\n",
    " - for training  'train-label-range.csv'\n",
    " - for testing   'test-label-range.csv'\n",
    "\n",
    "The following cell should be run only when dataset is ready or updated.\n",
    "\n",
    "※ You can use sound dataset stored in [sample guitar raw sound dataset](https://egstorageiotkitvol5.blob.core.windows.net/sound-ml-data/raw-sound-data.zip). please use a dataset that uploads a set of files that be extracted from the zip file to the blob which you create by your own Azure account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-3(onetime only)\n",
    "import os\n",
    "train_label_range_csv_file = 'train-label-range.csv'\n",
    "test_label_range_csv_file = 'test-label-range.csv'\n",
    "\n",
    "csv_files = [train_label_range_csv_file, test_label_range_csv_file]\n",
    "parsed_specs =[]\n",
    "for csv in csv_files:\n",
    "    specs ={}\n",
    "    parsed_specs.append(specs)\n",
    "    with open(csv,\"rt\") as f:\n",
    "        ranges = f.readlines()\n",
    "        for r in ranges:\n",
    "            spec = r.rstrip().split(',')\n",
    "            if not (spec[0] in specs):\n",
    "                specs[spec[0]] = []\n",
    "            specs[spec[0]].append([spec[1],spec[2]])\n",
    "\n",
    "duration_for_train = parsed_specs[0]\n",
    "duration_for_test = parsed_specs[1]\n",
    "            \n",
    "for i, ps in enumerate(parsed_specs):\n",
    "    print('spec for - ' + csv_files[i])\n",
    "    for d in duration_for_train.keys():\n",
    "        for s in duration_for_train[d]:\n",
    "            print(' {0}:{1}-{2}'.format(d,s[0],s[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download guitar sound dataset from your own blob container\n",
    "This code download only csv files that meet label specificated timespan criteria.\n",
    "Before you run following code, please set <i>source_azure_storage_account_connection_string</i> and <i>source_container_name</i> to match your storage account that contains the sound data files.\n",
    "The <i>source_azure_storage_account_connection_string</i> is configured into [data-storage-config.yml](./data-storage-config.yml) \n",
    "\n",
    "The files satisfied criteria will be downloaded in data and test folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-4 (onetime only)\n",
    "#!pip install -U azure-storage-blob>=12.2.0\n",
    "#!pip list\n",
    "import os\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "yml = {}\n",
    "with open(dataSrorageConfigFile,'r') as ymlfile:\n",
    "    yml.update(yaml.safe_load(ymlfile))\n",
    "#print('config - {}'.format(yml))\n",
    "\n",
    "# Specify datasource\n",
    "source_azure_storage_account_connection_string = yml['blob_connection_string']\n",
    "source_container_name = 'edgesounds'\n",
    "\n",
    "# Specify start and end time of duration for each chord\n",
    "# duration_for_train = {\n",
    "#    'major':[['2020/02/09 16:58:34', '2020/02/09 16:58:40'], ['2020/02/09 17:06:02','2020/02/09 17:06:10'], ['2020/02/09 16:59:42','2020/02/09 16:59:50'], ['2020/02/09 17:00:41','2020/02/09 17:00:49'],['2020/02/18 11:27:20', '2020/02/18 11:27:26'],['2020/02/18 11:28:05', '2020/02/18 11:28:11'],['2020/02/18 11:28:41', '2020/02/18 11:28:44'],['2020/02/18 11:29:18', '2020/02/18 11:29:20'],['2020/02/18 11:29:51', '2020/02/18 11:29:57'],['2020/02/18 11:30:25', '2020/02/18 11:30:29'],['2020/02/18 11:31:05', '2020/02/18 11:31:12']\n",
    "# ],\n",
    "#     'minor':[['2020/02/09 16:58:49', '2020/02/09 16:59:00'], ['2020/02/09 17:06:26','2020/02/09 17:06:36'], ['2020/02/09 16:59:57','2020/02/09 17:00:05'], ['2020/02/09 17:00:56','2020/02/09 17:01:03'],['2020/02/18 11:27:41', '2020/02/18 11:27:47'],['2020/02/18 11:28:22', '2020/02/18 11:28:26'],['2020/02/18 11:28:59', '2020/02/18 11:29:03'],['2020/02/18 11:29:33', '2020/02/18 11:29:38'],['2020/02/18 11:30:07', '2020/02/18 11:30:13'],['2020/02/18 11:30:45', '2020/02/18 11:30:49'],['2020/02/18 11:31:24', '2020/02/18 11:31:35']\n",
    "# ]\n",
    "# }\n",
    "\n",
    "# duration_for_test = {\n",
    "#     'major':[['2020/02/18 11:36:37', '2020/02/18 11:36:44'],['2020/02/18 11:37:12', '2020/02/18 11:37:18'],['2020/02/18 11:37:43', '2020/02/18 11:37:49'],['2020/02/18 11:38:18', '2020/02/18 11:38:23'],['2020/02/18 11:38:58', '2020/02/18 11:39:06'],['2020/02/18 11:39:36', '2020/02/18 11:39:40'],['2020/02/18 11:40:14', '2020/02/18 11:40:20']],\n",
    "#     'minor':[['2020/02/18 11:36:56', '2020/02/18 11:37:01'],['2020/02/18 11:37:25', '2020/02/18 11:37:33'],['2020/02/18 11:38:00', '2020/02/18 11:38:08'],['2020/02/18 11:38:35', '2020/02/18 11:38:42'],['2020/02/18 11:39:15', '2020/02/18 11:39:21'],['2020/02/18 11:39:53', '2020/02/18 11:39:59'],['2020/02/18 11:40:34', '2020/02/18 11:40:41']]\n",
    "# }\n",
    "\n",
    "def pickup_target_files(target_folder_name, duration_for_target):\n",
    "    folder_for_label = {}\n",
    "    condition_for_label = {}\n",
    "    target_data = {}\n",
    "    # data store for traning\n",
    "    data_folder = os.path.join(os.getcwd(), target_folder_name)\n",
    "    for dflk in duration_for_target.keys():\n",
    "        folder_for_key = os.path.join(data_folder, dflk)\n",
    "        folder_for_label[dflk] = folder_for_key\n",
    "        os.makedirs(folder_for_key, exist_ok=True)\n",
    "        condition_for_label[dflk] = []\n",
    "        durs = duration_for_target[dflk]\n",
    "        for dur in durs:\n",
    "            dur_se = []\n",
    "            while len(dur)>0:\n",
    "                t = dur.pop(0)\n",
    "                ttime = datetime.datetime.strptime(t, '%Y/%m/%d %H:%M:%S')\n",
    "                tnum = ttime.strftime('%Y%m%d%H%M%S') + '000000'\n",
    "                dur_se.append(int(tnum))\n",
    "            condition_for_label[dflk].append(dur_se)\n",
    "        target_data[dflk] = []\n",
    "    return folder_for_label, condition_for_label, target_data\n",
    "\n",
    "data_folder_name = 'data-wav'\n",
    "test_folder_name = 'test-wav'\n",
    "\n",
    "train_folder_for_label, train_condition_for_label, train_data = pickup_target_files(data_folder_name, duration_for_train)\n",
    "test_folder_for_label, test_condition_for_label, test_data = pickup_target_files(test_folder_name, duration_for_test)\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Connect to our blob via the BlobService\n",
    "blobServiceClient = BlobServiceClient.from_connection_string(source_azure_storage_account_connection_string)\n",
    "containerClient = blobServiceClient.get_container_client(source_container_name)\n",
    "\n",
    "def load_targeted_blobs(container, condition_for_target, folder_for_target, data_for_target ):\n",
    "    with open(soundDataDefFile, \"wb\") as ymlFile:\n",
    "        blobClient = containerClient.get_blob_client(soundDataDefFile)\n",
    "        stream = blobClient.download_blob()\n",
    "        blobContent = stream.readall()\n",
    "        ymlFile.write(blobContent)\n",
    "        \n",
    "    target_blobs = []\n",
    "    loaded_num_of_files = {}\n",
    "    for blob in containerClient.list_blobs():\n",
    "        matching = re.findall('sound-[0-9]+.wav', blob.name)\n",
    "        if len(matching)>0:\n",
    "            target_blobs.append({'blob':blob, 'num-of-ts':int(re.findall('[0-9]+',blob.name)[0])})\n",
    "    for l in condition_for_target.keys():\n",
    "        for cfl in condition_for_target[l]:\n",
    "            filtered = list(filter(lambda b: cfl[0] <= b['num-of-ts'] and b['num-of-ts'] <= cfl[1], target_blobs))\n",
    "            data_for_target[l].append(filtered)\n",
    "    \n",
    "    for l in data_for_target.keys():\n",
    "        num_of_files = 0\n",
    "        print('Label - '+l)\n",
    "        for dft in data_for_target[l]:\n",
    "            for ltd in dft:\n",
    "                blobClient = containerClient.get_blob_client(ltd['blob'])\n",
    "                stream = blobClient.download_blob()\n",
    "                wavFilePath = os.path.join(folder_for_target[l], ltd['blob'].name)\n",
    "                print(' Downloading - ' + ltd['blob'].name)\n",
    "                with open(wavFilePath,\"wb\") as wavFile:\n",
    "                    blobContent = stream.readall()\n",
    "                    wavFile.write(blobContent)\n",
    "                num_of_files = num_of_files + 1\n",
    "        loaded_num_of_files[l] = num_of_files\n",
    "\n",
    "    return loaded_num_of_files\n",
    "\n",
    "result = load_targeted_blobs(containerClient, train_condition_for_label, train_folder_for_label, train_data)\n",
    "for k in result.keys():\n",
    "    print('Loaded file for train:{0} - {1}'.format(k, result[k]))\n",
    "result = load_targeted_blobs(containerClient, test_condition_for_label, test_folder_for_label, test_data)\n",
    "for k in result.keys():\n",
    "    print('Loaded file for test:{0} - {1}'.format(k, result[k]))\n",
    "\n",
    "import shutil\n",
    "for fldr in [data_folder_name, test_folder_name]:\n",
    "    destFName = os.path.join(fldr,soundDataDefFile)\n",
    "    shutil.copy(soundDataDefFile, destFName)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test data\n",
    "reform data for training and test\n",
    "\n",
    "<i>local jupyternotebook execution enable l-5 (onetime only)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile loadwavsounds.py\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "def take_fft(wav_rawdata, sample_rate, fft, mels):\n",
    "    fdata = np.array([float(d) for d in wav_rawdata])\n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        y = fdata, sr = sample_rate,\n",
    "        n_fft = fft, n_mels = mels\n",
    "    )\n",
    "    return melspec, fdata\n",
    "\n",
    "def load_wavdata(wav_file_path, fft=2048, mels=128):\n",
    "#    print(wav_file_path)\n",
    "    wv = wave.open(wav_file_path,'rb')\n",
    "    wvinfo = wv.getparams()\n",
    "    fs, data = read(wav_file_path)\n",
    "    melspecs = []\n",
    "    rawdata = []\n",
    "    tdata = []\n",
    "    if (wvinfo.nchannels == 1):\n",
    "        tdata.append(data)\n",
    "    else:\n",
    "        tdata = data.T\n",
    "    if wvinfo.nchannels == len(tdata):\n",
    "        for data in tdata:\n",
    "            fftdata = take_fft(data, fs, fft, mels)\n",
    "            melspecs.append(fftdata[0])\n",
    "            rawdata.append(fftdata[1])\n",
    "    \n",
    "    result = {}\n",
    "    result['fft-data'] = librosa.power_to_db(melspecs, ref=np.max)\n",
    "    result['raw-data'] = rawdata\n",
    "    result['sampling-rate'] = fs\n",
    "    result['fft-freq'] = fft\n",
    "    result['fft-mels'] = mels\n",
    "    return result\n",
    "\n",
    "def load_csvdata(file):\n",
    "    m = load_wavdata(file)\n",
    "    return np.array(m[1]).T.astype(np.int16) / 32768\n",
    "\n",
    "# data_folder = 'data'\n",
    "# data_folder_path = os.path.join(os.getcwd(), data_folder)\n",
    "\n",
    "def load_data_definition(data_def_file_path):\n",
    "    definition = {}\n",
    "    with open(data_def_file_path, \"r\") as ymlFile:\n",
    "        yml = yaml.safe_load(ymlFile)\n",
    "        definition.update(yml)\n",
    "    return definition\n",
    "\n",
    "def reshape_dataset(sound_data, data_chunk):\n",
    "    dataset = np.zeros((len(sound_data), 1, 1, data_chunk))\n",
    "    for index, d in enumerate(sound_data):\n",
    "        dataset[index][0][0] = d\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_minimum_times(tdata):\n",
    "    shps = []\n",
    "    for l in tdata.keys():\n",
    "        for d in tdata[l]:\n",
    "            shps.append(d.shape[1])\n",
    "    return np.array(shps).min()\n",
    "\n",
    "\n",
    "def divid_data_by_minimum_shape(unit, dlen):\n",
    "    resultset = []\n",
    "    shp = unit.shape\n",
    "#    print('original shape - {}'.format(shp))\n",
    "    if shp[1] == dlen:\n",
    "        resultset.append(unit)\n",
    "    else:\n",
    "        pl = 0\n",
    "        while pl + dlen <= shp[1]:\n",
    "#            print('++pl={}'.format(pl))\n",
    "            divided = unit[:,pl:pl+dlen]\n",
    "            resultset.append(divided)\n",
    "#            print('divided shape - {}'.format(divided.shape))\n",
    "            pl = pl + dlen\n",
    "        if (shp[1] % dlen) != 0:\n",
    "            pl = shp[1]\n",
    "            while pl - dlen >= 0:\n",
    "#                print('--pl={}'.format(pl))\n",
    "                divided = unit[:,pl-dlen:pl]\n",
    "                resultset.append(divided)\n",
    "#                print('divided shape - {}'.format(divided.shape))\n",
    "                pl = pl - dlen\n",
    "    \n",
    "    return np.array(resultset)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_folder_path, fft=2048, mels=128, minimum=0):\n",
    "    tbdata ={}\n",
    "    rate = 0\n",
    "    num_of_data = 0\n",
    "    for df in os.listdir(path=data_folder_path):\n",
    "        if os.path.isfile(os.path.join(data_folder_path, df)):\n",
    "            continue\n",
    "            \n",
    "        tbdata[df] = []\n",
    "        ldata_folder_path = os.path.join(data_folder_path,df)\n",
    "        for datafile in os.listdir(path=ldata_folder_path):\n",
    "            datafile_path = os.path.join(ldata_folder_path, datafile)\n",
    "            fftwav = load_wavdata(datafile_path, fft=fft, mels=mels)\n",
    "            tbdata[df].extend(fftwav['fft-data'])\n",
    "            fft = fftwav['fft-freq']\n",
    "            mels = fftwav['fft-mels']\n",
    "            rate = fftwav['sampling-rate']\n",
    "    \n",
    "    mintsize = get_minimum_times(tbdata)\n",
    "    if mintsize < minimum:\n",
    "        print('minimum time range of dataset is {} but select minimum arg value:{} '.format(mintsize, minimum))\n",
    "        mintsize = minimum\n",
    "    else:\n",
    "        print('minimum time range of dataset is {}'.format(mintsize))\n",
    "    tdata ={}\n",
    "    num_of_data = 0\n",
    "    for l in tbdata.keys():\n",
    "        tdata[l] = []\n",
    "#        print('label:{}'.format(l))\n",
    "#        index = 0\n",
    "        for u in tbdata[l]:\n",
    "#            print('index:{}'.format(index))\n",
    "#            index = index +1\n",
    "            if u.shape[1] < mintsize:\n",
    "                print('this data is too short - {}'.format(u.shape))\n",
    "                continue\n",
    "            divided = divid_data_by_minimum_shape(u, mintsize)\n",
    "            tdata[l].extend(divided)\n",
    "            num_of_data = num_of_data + len(divided)\n",
    "\n",
    "    data_of_sounds = np.zeros((num_of_data,1, mels, mintsize),dtype=np.float)\n",
    "    label_of_sounds = np.zeros(num_of_data,dtype=int)\n",
    "    label_matrix_of_sounds = np.zeros((num_of_data, len(tdata.keys())))\n",
    "    labelname_of_sounds = np.empty(num_of_data,dtype=object)\n",
    "    index = 0\n",
    "    lindex = 0\n",
    "    labeling_for_train = {}\n",
    "    for l in tdata.keys():\n",
    "        for fftdata in tdata[l]:\n",
    "            data_of_sounds[index][0] = fftdata\n",
    "            label_of_sounds[index] = lindex\n",
    "            label_matrix_of_sounds[index, lindex] = 1.\n",
    "            labelname_of_sounds[index] = l\n",
    "            index = index + 1\n",
    "        labeling_for_train[l] = lindex\n",
    "        lindex = lindex + 1\n",
    "\n",
    "    print('num_of_data={},size of data_of_sounds={}'.format(num_of_data,len(data_of_sounds)))\n",
    "    indexx = np.arange(num_of_data)\n",
    "    random.shuffle(indexx)\n",
    "\n",
    "    data_of_sounds = data_of_sounds[indexx]\n",
    "    label_matrix_of_sounds = label_matrix_of_sounds[indexx]\n",
    "    label_of_sounds = label_of_sounds[indexx]\n",
    "    labelname_of_sounds = labelname_of_sounds[indexx]\n",
    "\n",
    "    # dataShape = data_of_sounds[0].shape\n",
    "    # data_of_sounds = data_of_sounds.reshape(len(data_of_sounds), dataShape[1], dataShape[2], dataShape[0])\n",
    "\n",
    "    # train_dataset is labeled sound data set\n",
    "    train_dataset = [data_of_sounds, label_of_sounds, labelname_of_sounds, rate, fft, mels]\n",
    "    \n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check sound data for training \n",
    "You can confirm that content of sound data by following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-6\n",
    "from loadwavsounds import load_data\n",
    "import librosa, librosa.display\n",
    "\n",
    "mels = 256\n",
    "fft_freq = 4096\n",
    "data_def_file = 'sounddata.yml'\n",
    "print('loading train data...')\n",
    "train_dataset = load_data(data_folder_path, fft=fft_freq, mels=mels)\n",
    "\n",
    "print('loading test data...')\n",
    "test_dataset = load_data(test_folder_path, fft=fft_freq, mels=mels, minimum=train_dataset[0].shape[3])\n",
    "\n",
    "print(\"Loaded {} of train data and {} of test data\".format(len(train_dataset[0]),len(test_dataset[0])))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#gx = np.arange(1,data_chunk,1)\n",
    "sample_size = 9\n",
    "#for d in train_dataset[0]:\n",
    "#    print('data shape - {}'.format(d.shape))\n",
    "    \n",
    "figure = plt.figure(figsize=(16,16))\n",
    "print('rate={}'.format(len(train_dataset)))\n",
    "for d in range(0,sample_size):\n",
    "    plt.subplot(3,3,d+1)\n",
    "    librosa.display.specshow(train_dataset[0][d][0],x_axis='time', y_axis='mel', fmax=train_dataset[3])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(train_dataset[2][d])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data store for training on remote computer\n",
    "This logic execution is necessary for only when data is updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "# retrieve current datastore in the workspace\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Upload files to dataset on datastore\n",
    "datastore.upload(src_dir='data',\n",
    "                 target_path= dataset_name,\n",
    "                 overwrite=True,\n",
    "                 show_progress=True)\n",
    "datastore.upload(src_dir='test',\n",
    "                 target_path= testset_name,\n",
    "                 overwrite=True,\n",
    "                 show_progress=True)\n",
    "\n",
    "# create a FileDataset pointing to files in 'animals' folder and its subfolders recursively\n",
    "datastore_paths = [(datastore, dataset_name)]\n",
    "sound_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "teststore_paths = [(datastore, testset_name)]\n",
    "sound_ts = Dataset.File.from_files(path=teststore_paths)\n",
    "\n",
    "print(sound_ds)\n",
    "\n",
    "# Register dataset to current workspace\n",
    "sound_dataset = sound_ds.register(workspace=ws,\n",
    "                                 name=dataset_name,\n",
    "                                 description='sound classification training data')\n",
    "sound_testset = sound_ts.register(workspace=ws,\n",
    "                                 name=testset_name,\n",
    "                                 description='sound classification test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct CNN model\n",
    "\n",
    "Following cell show a sample of CNN model for sound classification and training.\n",
    "The cell logic will be run on this computing environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-7\n",
    "# https://www.tensorflow.org/tutorials/images/intro_to_cnns?hl=ja\n",
    "import os\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "print('tensorflow version - '+tf.__version__)\n",
    "\n",
    "dataShape = train_dataset[0][0].shape\n",
    "# データを、reshape(256,5,1)すればいいらしい。\n",
    "print('train data shape is - {}'.format(dataShape))\n",
    "train_ds = train_dataset[0].reshape(len(train_dataset[0]),dataShape[1],dataShape[2],dataShape[0])\n",
    "\n",
    "tdataShape = test_dataset[0][0].shape\n",
    "print('test data shape is - {}'.format(tdataShape))\n",
    "test_ds = test_dataset[0].reshape(len(test_dataset[0]), dataShape[1],dataShape[2],dataShape[0])\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16,input_shape=(dataShape[1],dataShape[2],1),kernel_size=(8,1),padding='same', strides=(4,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1,2), padding='same'))\n",
    "model.add(layers.Conv2D(filters=16,input_shape=(1,128,5),kernel_size=(8, 1),padding='same', strides=(4,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1,2), padding='same'))\n",
    "model.add(layers.Conv2D(filters=16,input_shape=(1, 16,5),kernel_size=(8,1),padding='same', strides=(4,1), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1,2), padding='same'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Above code part is same as training logic on remote computer\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(train_ds, train_dataset[1], epochs=5, validation_data=(test_ds, test_dataset[1]))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds,test_dataset[1])\n",
    "print('Test accuracy - '+str(test_acc))\n",
    "\n",
    "for hk in hist.history.keys():\n",
    "    print(hk)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_list = list(range(1, len(hist.history['accuracy']) + 1))\n",
    "plt.plot(epoch_list, hist.history['accuracy'], epoch_list, hist.history['val_accuracy'])\n",
    "plt.legend(('Training Accuracy', \"Validation Accuracy\"))\n",
    "plt.show()\n",
    "\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "# Plot a random sample of 10 test images, their predicted labels and ground truth\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "for i, index in enumerate(np.random.choice(test_dataset[1], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.plot(test_dataset[0][index][0][0])\n",
    "    librosa.display.specshow(test_dataset[0][index][0],x_axis='time', y_axis='mel', fmax=test_dataset[3])\n",
    "    predict_index = np.argmax(predictions[index])\n",
    "    true_index = np.argmax(test_dataset[1][index])\n",
    "    # print('{}-{}'.format(predict_index, true_index))\n",
    "    # Set the title for each image\n",
    "    ax.set_title(\"{} ({})\".format(test_dataset[2][predict_index], \n",
    "                                  test_dataset[2][true_index]),\n",
    "                                  color=(\"green\" if predict_index == true_index else \"red\"))\n",
    "\n",
    "\n",
    "# Save learned model as .pkl and .h5\n",
    "model_output_path = 'outputs'\n",
    "os.makedirs(model_output_path, exist_ok=True)\n",
    "save_model_name = 'sound-classification-model'\n",
    "save_model_name_ext = ['pkl','h5']\n",
    "for ext in save_model_name_ext:\n",
    "    save_model_path_name = save_model_name + \".\" + ext\n",
    "    save_model_path = os.path.join(model_output_path, save_model_path_name)\n",
    "    print('Saving learned model as {}'.format(save_model_path))\n",
    "    model.save(save_model_path)\n",
    "\n",
    "# Save target data config in sounddata.yml\n",
    "yml = {}\n",
    "with open(soundDataDefFile,'r') as ymlfile:\n",
    "    yml.update(yaml.safe_load(ymlfile))\n",
    "yml['data-width'] = dataShape[2]\n",
    "yml['fft-freq'] = fft_freq\n",
    "yml['fft-mels'] = mels\n",
    "with open(soundDataDefFile, 'w') as ymlfile:\n",
    "    yaml.dump(yml, ymlfile)\n",
    "print('data width is saved in {}'.format(soundDataDefFile))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"sklearn-script\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model traing script\n",
    "The following cell is the script of model definition, model training for running on remote computing cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "from loadwavsounds import load_data\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--test-folder', type=str, dest='test_folder', help='test folder mounting point')\n",
    "#parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = args.data_folder\n",
    "test_folder = args.test_folder\n",
    "print('Data folder:{0}, Test folder:{1}'.format( data_folder, test_folder))\n",
    "\n",
    "data_def_file = 'sounddata.yml'\n",
    "\n",
    "train_dataset = load_data(data_folder,data_def_file)\n",
    "test_dataset = load_data(test_folder, data_def_file)\n",
    "mels = 256\n",
    "fft_freq = 4096\n",
    "print('loading train data...')\n",
    "train_dataset = load_data(data_folder, fft=fft_freq, mels=mels)\n",
    "\n",
    "print('loading test data...')\n",
    "test_dataset = load_data(test_folder, fft=fft_freq, mels=mels, minimum=train_dataset[0].shape[3])\n",
    "\n",
    "print(\"Loaded {} of train data and {} of test data\".format(len(train_dataset[0]),len(test_dataset[0])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "print('tensorflow version - '+tf.__version__)\n",
    "\n",
    "dataShape = train_dataset[0][0].shape\n",
    "# データを、reshape(256,5,1)すればいいらしい。\n",
    "print('train data shape is - {}'.format(dataShape))\n",
    "train_ds = train_dataset[0].reshape(len(train_dataset[0]),dataShape[1],dataShape[2],dataShape[0])\n",
    "\n",
    "tdataShape = test_dataset[0][0].shape\n",
    "print('test data shape is - {}'.format(tdataShape))\n",
    "test_ds = test_dataset[0].reshape(len(test_dataset[0]), dataShape[1],dataShape[2],dataShape[0])\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16,input_shape=(dataShape[1],dataShape[2],1),kernel_size=(8,1),padding='same', strides=(4,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1,2), padding='same'))\n",
    "model.add(layers.Conv2D(filters=16,input_shape=(1,128,5),kernel_size=(8, 1),padding='same', strides=(4,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1,2), padding='same'))\n",
    "model.add(layers.Conv2D(filters=16,input_shape=(1, 16,5),kernel_size=(8,1),padding='same', strides=(4,1), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1,2), padding='same'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Above code part is same as training logic on remote computer\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(train_ds, train_dataset[1], epochs=5, validation_data=(test_ds, test_dataset[1]))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds,test_dataset[1])\n",
    "print('Test accuracy - '+str(test_acc))\n",
    "\n",
    "for hk in hist.history.keys():\n",
    "    print(hk)\n",
    "    \n",
    "# Save learned model as .pkl and .h5\n",
    "model_output_path = 'outputs'\n",
    "os.makedirs(model_output_path, exist_ok=True)\n",
    "save_model_name = 'sound-classification-model'\n",
    "save_model_name_ext = ['pkl','h5']\n",
    "for ext in save_model_name_ext:\n",
    "    save_model_path_name = save_model_name + \".\" + ext\n",
    "    save_model_path = os.path.join(model_output_path, save_model_path_name)\n",
    "    print('Saving learned model as {}'.format(save_model_path))\n",
    "    model.save(save_model_path)\n",
    "\n",
    "# Save target data config in sounddata.yml\n",
    "yml = {}\n",
    "with open(soundDataDefFile,'r') as ymlfile:\n",
    "    yml.update(yaml.safe_load(ymlfile))\n",
    "yml['data-width'] = dataShape[2]\n",
    "yml['fft-mels'] = mels\n",
    "yml['fft-freq'] = fft-freq\n",
    "with open(soundDataDefFile, 'w') as ymlfile:\n",
    "    yaml.dump(yml, ymlfile)\n",
    "print('data width is saved in {}'.format(soundDataDefFile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('loadwavsounds.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Learning!\n",
    "train CNN model with sound dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('my_env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk','scikit-learn','azureml-dataprep[pandas,fuse]>=1.1.14','tensorflow==2.1.0','matplotlib', 'librosa', 'pyyaml'])\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core import Dataset, Run\n",
    "\n",
    "# Get a dataset by name\n",
    "sound_dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "data_mount = sound_dataset.as_named_input('sound_data').as_mount()\n",
    "test_dataset = Dataset.get_by_name(workspace=ws, name=testset_name)\n",
    "test_mount = sound_dataset.as_named_input('sound_test').as_mount()\n",
    "\n",
    "script_params = {\n",
    "    # to mount files referenced by mnist dataset\n",
    "    '--data-folder': data_mount,\n",
    "    '--test-folder': test_mount\n",
    "}\n",
    "\n",
    "est = SKLearn(source_directory=script_folder,\n",
    "              script_params=script_params,\n",
    "              compute_target=compute_target,\n",
    "              environment_definition=env,\n",
    "              entry_script='train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For debug\n",
    "Following three blocks are used to check specified parameters and dataset validity.\n",
    "When you don't need to debug, please go forward to [\"Submit the job to the cluster\"](#Submit-the-job-to-the-cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/testargs.py\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--test-folder', type=str, dest='test_folder', help='test folder mounting point')\n",
    "#parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = args.data_folder\n",
    "test_folder = args.test_folder\n",
    "print('Data folder:{0}', 'Test folder:{1}'.format( data_folder, test_folder))\n",
    "\n",
    "import os\n",
    "chdir = os.getcwd()\n",
    "print('Current Dir - '+chdir)\n",
    "folders = {'data':data_folder,'test':test_folder}\n",
    "for fld in folders.keys():\n",
    "    cfld = folders[fld]\n",
    "    print('Check content of {0} - {1}'.format(fld, cfld))\n",
    "    for f in os.listdir(cfld):\n",
    "        print(' '+f)\n",
    "        cdir = os.path.join(data_folder, f)\n",
    "        if os.path.isdir(cdir):\n",
    "            for cf in os.listdir(cdir):\n",
    "                print('  '+cf)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('my_env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk','matplotlib','azureml-dataprep[pandas,fuse]>=1.1.14'])\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core import Dataset, Run\n",
    "\n",
    "# Get a dataset by name\n",
    "sound_dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "data_mount = sound_dataset.as_named_input('sound_data').as_mount()\n",
    "test_dataset = Dataset.get_by_name(workspace=ws, name=testset_name)\n",
    "test_mount = sound_dataset.as_named_input('sound_test').as_mount()\n",
    "print(data_mount)\n",
    "print(test_mount)\n",
    "script_params = {\n",
    "    # to mount files referenced by mnist dataset\n",
    "    '--data-folder': data_mount,\n",
    "    '--test-folder': test_mount\n",
    "}\n",
    "\n",
    "est = SKLearn(source_directory=script_folder,\n",
    "              script_params=script_params,\n",
    "              compute_target=compute_target,\n",
    "              environment_definition=env,\n",
    "              entry_script='testargs.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-8\n",
    "import numpy as np\n",
    "from loadwavsounds import load_wavdata, divid_data_by_minimum_shape, load_data_definition\n",
    "import os\n",
    "import random\n",
    "\n",
    "#data_def_file = 'sounddata.yml'\n",
    "#datafile = 'cherry-sound-20200218113643319806.csv'\n",
    "data_def = load_data_definition(data_def_file)\n",
    "#csv_dataset = parse_file(datafile,np.array([]),data_chunk)\n",
    "#sound_dataset = np.zeros((csv_dataset[0], 1, 1, data_chunk))\n",
    "#index = 0\n",
    "#for d in csv_dataset[1]:\n",
    "#    sound_dataset[index][0][0] = d\n",
    "#    index = index + 1\n",
    "test_data_files = []\n",
    "for d in os.listdir(test_folder_path):\n",
    "    dname = os.path.join(test_folder_path, d)\n",
    "    if os.path.isdir(dname):\n",
    "        for f in os.listdir(dname):\n",
    "            if f.rfind('.wav') >= 0:\n",
    "                test_data_files.append(os.path.join(dname,f))\n",
    "random.shuffle(test_data_files)\n",
    "mels = data_def['fft-mels']\n",
    "dataWidth = data_def['data-width']\n",
    "fft=data_def['fft-freq']\n",
    "\n",
    "print('load data by fft={},mels={},data-width={}'.format(fft, mels, dataWidth))\n",
    "wavdata = load_wavdata(test_data_files[0], fft=fft, mels=mels)\n",
    "wav_dataset = divid_data_by_minimum_shape(wavdata['fft-data'][0], dataWidth)\n",
    "print('wav_dataset.shape - {}, len={}'.format(wav_dataset.shape, len(wav_dataset)))\n",
    "data_of_sounds = np.zeros((len(wav_dataset),1, mels, dataWidth), dtype=np.float)\n",
    "for index, wd in enumerate(wav_dataset):\n",
    "    data_of_sounds[index][0] = wd\n",
    "    \n",
    "dataShape = data_of_sounds[0].shape\n",
    "data_of_sounds = data_of_sounds.reshape(len(data_of_sounds),dataShape[1],dataShape[2],dataShape[0])\n",
    "print('test data shape - {}'.format(data_of_sounds.shape))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "print('tensorflow version - '+tf.__version__)\n",
    "\n",
    "#model_file_path ='outputs/sound-classification-model.h5'\n",
    "model_folder_path = 'outputs'\n",
    "model_file_name = 'sound-classification-model.pkl'\n",
    "model_file_path = os.path.join(model_folder_path, model_file_name)\n",
    "# model name should be used other style\n",
    "model = models.load_model(model_file_path)\n",
    "\n",
    "predicted = model.predict(data_of_sounds)\n",
    "result = predicted.tolist()\n",
    "for r in result:\n",
    "    print('{0}<->{1}'.format(r[0],r[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "Run the experiment by submitting the estimator object. And you can navigate to Azure portal to monitor the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies (`conda_packages` parameter in the above estimator constructor) don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n",
    "\n",
    "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n",
    "\n",
    "### Jupyter widget\n",
    "\n",
    "Watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True) # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are you happy with the model??? Register it in Azure Machine Learning to manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='sound_clasification_model', model_path='outputs/')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "In this Azure Machine Learning tutorial, you used Python to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Access and examine the data\n",
    "> * Train multiple models on a remote cluster using the tensorflow keras machine learning library\n",
    "> * Review training details and register the best model\n",
    "\n",
    "You are ready to deploy this registered model using the instructions in the next part of the tutorial series:\n",
    "\n",
    "> [Tutorial 2 - Deploy models](ai-sound-major-miner-classification-part2-deploy.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
