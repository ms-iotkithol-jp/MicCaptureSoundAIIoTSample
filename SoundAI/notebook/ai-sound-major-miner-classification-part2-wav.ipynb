{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #2:  Deploy an sound classification model in Azure Container Instance (ACI)\n",
    "\n",
    "This tutorial is **part two of a two-part tutorial series**. In the [previous tutorial](ai-sound-major-miner-classification-part1.ipynb), you trained machine learning models and then registered a model in your workspace on the cloud.  \n",
    "\n",
    "Now, you're ready to deploy the model as a web service in [Azure Container Instances](https://docs.microsoft.com/azure/container-instances/) (ACI). A web service is an image, in this case a Docker image, that encapsulates the scoring logic and the model itself. \n",
    "\n",
    "In this part of the tutorial, you use Azure Machine Learning service (Preview) to:\n",
    "\n",
    "> * Set up your testing environment\n",
    "> * Retrieve the model from your workspace\n",
    "> * Test the model locally\n",
    "> * Deploy the model to ACI\n",
    "> * Test the deployed model\n",
    "\n",
    "ACI is a great solution for testing and understanding the workflow. For scalable production deployments, consider using Azure Kubernetes Service. For more information, see [how to deploy and where](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where).\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete the model training in the [Tutorial #1: Train an sound classification model with Azure Machine Learning](ai-sound-major-miner-classification-part1.ipynb) notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Start by setting up a testing environment.\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import the Python packages needed for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import azureml.core\n",
    "\n",
    "# display the core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the model\n",
    "\n",
    "You registered a model in your workspace in the previous tutorial. Now, load this workspace and download the model to your local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "import os \n",
    "ws = Workspace.from_config()\n",
    "model=Model(ws, 'sound_clasification_model')\n",
    "model.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# verify the downloaded model file\n",
    "#model_file_path ='outputs/sound-classification-model.h5'\n",
    "model_folder_path = 'outputs'\n",
    "model_file_name = 'sound-classification-model.pkl'\n",
    "model_file_path = os.path.join(model_folder_path, model_file_name)\n",
    "model_file_path = os.path.join(os.getcwd(), model_file_path)\n",
    "\n",
    "os.stat(model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model locally\n",
    "\n",
    "Before deploying, make sure your model is working locally by:\n",
    "* Loading test data\n",
    "* Predicting test data\n",
    "* Examining the confusion matrix\n",
    "\n",
    "### Load test data\n",
    "\n",
    "Load the test data from the **./data/** directory created during the training tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_wavsounds import load_data_definition, load_wavdata\n",
    "\n",
    "testset_name = 'sound_test'\n",
    "test_folder_path = 'test'\n",
    "\n",
    "data_def_file = 'sounddata.yml'\n",
    "data_def = load_data_definition(data_def_file)\n",
    "mels = data_def['fft-mels']\n",
    "dataWidth = data_def['data-width']\n",
    "fft=data_def['fft-freq']\n",
    "\n",
    "# data_chunk shoud be same value for sound capturing chunk\n",
    "print('loading test data...')\n",
    "test_dataset = load_wavdata(test_folder_path, fft=fft, mels=mels, minimum=dataWidth)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#gx = np.arange(1,data_chunk,1)\n",
    "\n",
    "sample_size = 9\n",
    "#for d in train_dataset[0]:\n",
    "#    print('data shape - {}'.format(d.shape))\n",
    "    \n",
    "figure = plt.figure(figsize=(16,16))\n",
    "for d in range(0,sample_size):\n",
    "    plt.subplot(3,3,d+1)\n",
    "    librosa.display.specshow(test_dataset[0][d][0],x_axis='time', y_axis='mel', fmax=test_dataset[3])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(test_dataset[2][d])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test data\n",
    "\n",
    "Feed the test dataset to the model to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "print('tensorflow version - '+tf.__version__)\n",
    "\n",
    "learned_model = models.load_model(model_file_path)\n",
    "print('Loaded - {}'.format(model_file_path))\n",
    "\n",
    "tdataShape = test_dataset[0][0].shape\n",
    "print('test data shape is - {}'.format(tdataShape))\n",
    "test_ds = test_dataset[0].reshape(len(test_dataset[0]), dataShape[1],dataShape[2],dataShape[0])\n",
    "\n",
    "predicted = learned_model.predict(test_ds)\n",
    "print('Prediction done for test data - {}'.format(predicted.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Examine the confusion matrix\n",
    "\n",
    "Generate a confusion matrix to see how many samples from the test set are classified correctly. Notice the mis-classified value for the incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "label_index = {}\n",
    "for index,l in enumerate(test_dataset[2]):\n",
    "    if l in label_index:\n",
    "        if label_index[l] != test_dataset[1][index]:\n",
    "            print('test data has been broken!')\n",
    "            break\n",
    "    else:\n",
    "        label_index[l] = test_dataset[1][index]\n",
    "print('label_index - {}'.format(label_index))\n",
    "\n",
    "labeled_predicted = []\n",
    "for p in predicted:\n",
    "    for lk in label_index.keys():\n",
    "        if p.argmax() == label_index[lk]:\n",
    "            labeled_predicted.append(lk)\n",
    "            break\n",
    "print('labeled prediction - done.')\n",
    "            \n",
    "#print('test_dataset[0]  - {}'.format(test_dataset[1][10]))\n",
    "conf_mx = confusion_matrix(test_dataset[2], labeled_predicted)\n",
    "print(conf_mx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matplotlib to display the confusion matrix as a graph. In this graph, the X axis represents the actual values, and the Y axis represents the predicted values. The color in each grid represents the error rate. The lighter the color, the higher the error rate is. For example, many 5's are mis-classified as 3's. Hence you see a bright grid at (5,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the diagonal cells so that they don't overpower the rest of the cells when visualized\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(norm_conf_mx, cmap=plt.cm.bone)\n",
    "ticks = np.arange(0, len(label_index.keys()), 1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(ticks)\n",
    "ax.set_yticklabels(ticks)\n",
    "fig.colorbar(cax)\n",
    "plt.ylabel('true labels', fontsize=14)\n",
    "plt.xlabel('predicted values', fontsize=14)\n",
    "plt.savefig('conf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export trained model for Edge by hdf5 format\n",
    "Please see [How to try AI on Edge](https://github.com/ms-iotkithol-jp/MicCaptureIoTSoundSample#step-2---%E3%82%AF%E3%83%A9%E3%82%A6%E3%83%89%E4%B8%8A%E3%81%A7%E3%81%AE%E9%9F%B3%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9Fai%E3%81%AE%E6%A7%8B%E7%AF%89%E3%81%A8%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BFai%E3%81%AE%E5%88%A9%E7%94%A8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_folder_name = 'toedge'\n",
    "\n",
    "if not os.path.exists(edge_folder_name):\n",
    "    os.mkdir(edge_folder_name)\n",
    "\n",
    "model_for_edge_file_path = os.path.splitext(model_file_path)[0] + '.h5'\n",
    "model_for_edge_file_path = os.path.join(edge_folder_name,os.path.basename(model_for_edge_file_path) )\n",
    "#print('file path of model for edge - {}'.format(model_for_edge_file_path))\n",
    "\n",
    "learned_model.save(model_for_edge_file_path)\n",
    "\n",
    "labels_def_file_name = 'labels.txt'\n",
    "with open(os.path.join(edge_folder_name, labels_def_file_name),'w', newline='\\n') as lf:\n",
    "    for l in label_index.keys():\n",
    "        lf.write('{}\\n'.format(l))\n",
    "\n",
    "print('Download {} and {} to AI on Edge from {}'.format(os.path.basename(model_for_edge_file_path), labels_def_file_name, edge_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Once you've tested the model and are satisfied with the results, deploy the model as a web service hosted in ACI. \n",
    "\n",
    "To build the correct environment for ACI, provide the following:\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the ACI\n",
    "* The model you trained before\n",
    "\n",
    "### Create scoring script\n",
    "\n",
    "Create the scoring script, called score.py, used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
    "\n",
    "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from loadwavsounds import load_wavdata, divid_data_by_minimum_shape, load_data_definition, take_fft\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sound-classification-model.pkl')\n",
    "    model = models.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    data_json = json.load(raw_data)\n",
    "    wavdata = np.array(data_json['data'])\n",
    "    sample_rate = data_json['sample-rate']\n",
    "    mels = data_json['fft-mels']\n",
    "    fft = data_json['fft-freq']\n",
    "    dataWidth = data_json['data-width']\n",
    "\n",
    "    predicted_results ={}\n",
    "    for channel, data in enumerate(wavdata):\n",
    "        fft_data = take_fft(data, sample_rate, fft, mels)\n",
    "        wav_dataset = divid_data_by_minimum_shape(fft_data, dataWidth)\n",
    "        data_of_sounds = np.zeros((len(wav_dataset),1, mels, dataWidth), dtype=np.float)\n",
    "        for index, wd in enumerate(wav_dataset):\n",
    "            data_of_sounds[index][0] = wd\n",
    "    \n",
    "        dataShape = data_of_sounds[0].shape\n",
    "        data_of_sounds = data_of_sounds.reshape(len(data_of_sounds),dataShape[1],dataShape[2],dataShape[0])\n",
    "\n",
    "        predicted = self.model.predict(data_of_sounds)\n",
    "        result = predicted.tolist()\n",
    "        predicted_results[channel] = result\n",
    "\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "    return predicted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment file\n",
    "\n",
    "Next, create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs `scikit-learn` and `azureml-sdk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "#myenv.add_conda_package(\"sound_classification\")\n",
    "myenv.add_pip_package(\"azureml-defaults\")\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of the `myenv.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create configuration file\n",
    "\n",
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"guitar-sound\",  \"method\" : \"tensorflow.keras\"}, \n",
    "                                               description='Predict major minor sound classification with CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy in ACI\n",
    "Estimated time to complete: **about 7-8 minutes**\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "1. Create environment object containing dependencies needed by the model using the environment file (`myenv.yml`)\n",
    "1. Create inference configuration necessary to deploy the model as a web service using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * envrionment object created in previous step\n",
    "1. Deploy the model to the ACI container.\n",
    "1. Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "service_name = 'sound-classification-svc'\n",
    "\n",
    "# Remove any existing service under the same name.\n",
    "try:\n",
    "    Webservice(ws, service_name).delete()\n",
    "except WebserviceException:\n",
    "    pass\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name=service_name, \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follwoing part has not been tested. sorry\n",
    "\n",
    "## Test deployed service\n",
    "\n",
    "Earlier you scored all the test data with the local version of the model. Now, you can test the deployed model with a random sample of 30 images from the test data.  \n",
    "\n",
    "The following code goes through these steps:\n",
    "1. Send the data as a JSON array to the web service hosted in ACI. \n",
    "\n",
    "1. Use the SDK's `run` API to invoke the service. You can also make raw calls using any HTTP tool such as curl.\n",
    "\n",
    "1. Print the returned predictions and plot them along with the input images. Red font and inverse image (white on black) is used to highlight the misclassified samples. \n",
    "\n",
    " Since the model accuracy is high, you might have to run the following code a few times before you can see a misclassified sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "from loadwavsounds import load_data_definition\n",
    "\n",
    "data_def_file = 'sounddata.yml'\n",
    "\n",
    "print('loading data definition...')\n",
    "data_def = load_data_definition(data_def_file)\n",
    "\n",
    "test_folder_path = 'test'\n",
    "test_files = []\n",
    "for lf in os.listdir(test_folder_path):\n",
    "    if os.path.isdir(lf):\n",
    "        for tf in os.listdir(lf):\n",
    "            if os.path.isfile(tf):\n",
    "                if (tf.rfind('.wav')):\n",
    "                    test_files.append(os.path.join(lf,if))\n",
    "random.shuffle(test_files)\n",
    "print('loading test data...')\n",
    "\n",
    "wav_file_path = test_files[0]\n",
    "wv = wave.open(wav_file_path,'rb')\n",
    "wvinfo = wv.getparams()\n",
    "fs, data = read(wav_file_path)\n",
    "sounds = []\n",
    "if wvinfo.nchannels == 1:\n",
    "    sounds.append(data)\n",
    "else:\n",
    "    tdata = data.T\n",
    "    for w in tdata:\n",
    "        sounds.append(w)\n",
    "\n",
    "test_samples = json.dumps({\"data\": sounds, \"sample-rate\": fs, \"fft-freq\": data_def['fft-freq'], \"fft-mels\": data_def['fft-mels'], \"data-width\": data_def['data-width']})\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also send raw HTTP request to test the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(test_dataset[0])-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(test_dataset[0][random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# for AKS deployment you'd need to the service key in the header as well\n",
    "# api_key = service.get_key()\n",
    "# headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "#print(\"input data:\", input_data)\n",
    "print(\"label:\", test_dataset[3][random_index])\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "To keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
